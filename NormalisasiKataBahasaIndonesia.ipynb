{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ko4sG8tRaghl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gu9zDMrYkWei",
        "outputId": "2ec34c7d-99da-44b7-c211-08f00babb4a1"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('kamus_clean.csv')"
      ],
      "metadata": {
        "id": "cYgsyQ07l6dc"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Tokenization\n",
        "def tokenize(text):\n",
        "    return word_tokenize(text.lower())\n",
        "\n",
        "df['tokens'] = df['TIDAK BAKU'].apply(tokenize)"
      ],
      "metadata": {
        "id": "6ExFj4xcl-8M"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Stopword Removal\n",
        "stop_words = set(stopwords.words('indonesian'))\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    return [word for word in tokens if word not in stop_words]\n",
        "\n",
        "df['filtered_tokens'] = df['tokens'].apply(remove_stopwords)"
      ],
      "metadata": {
        "id": "g6l4lNe7mDxu"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Optimized Stemming\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "# Caching for stemmed words\n",
        "stem_cache = {}\n",
        "\n",
        "def cached_stem(word):\n",
        "    if word not in stem_cache:\n",
        "        stem_cache[word] = stemmer.stem(word)\n",
        "    return stem_cache[word]\n",
        "\n",
        "def stem_words_cached(tokens):\n",
        "    return [cached_stem(word) for word in tokens]\n",
        "\n",
        "# Apply stemming with progress bar\n",
        "tqdm.pandas(desc=\"Stemming\")\n",
        "df['stemmed_tokens'] = df['filtered_tokens'].progress_apply(stem_words_cached)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FTctm5bmGLc",
        "outputId": "04a051fd-dba1-4857-8710-b263eee68d0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stemming:   3%|â–Ž         | 495/15022 [01:15<22:55, 10.56it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Index Formation\n",
        "index = set()\n",
        "for tokens in df['stemmed_tokens']:\n",
        "    index.update(tokens)"
      ],
      "metadata": {
        "id": "-RK9Bm8OnPsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. TF-IDF Calculation\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['BAKU'])"
      ],
      "metadata": {
        "id": "eYk6DJqrnUC1"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Query Matching\n",
        "def search_query(query):\n",
        "    query_vector = tfidf_vectorizer.transform([query])\n",
        "    cosine_similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
        "    related_docs_indices = cosine_similarities.argsort()[:-6:-1]\n",
        "    return df.iloc[related_docs_indices]"
      ],
      "metadata": {
        "id": "ynE6OZAenWHP"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "print(\"Tokenized words:\")\n",
        "print(df['tokens'].head())\n",
        "print(\"\\nFiltered words:\")\n",
        "print(df['filtered_tokens'].head())\n",
        "print(\"\\nStemmed words:\")\n",
        "print(df['stemmed_tokens'].head())\n",
        "print(\"\\nIndex (first 10 terms):\")\n",
        "print(list(index)[:10])\n",
        "print(\"\\nTF-IDF Matrix shape:\")\n",
        "print(tfidf_matrix.shape)\n",
        "print(\"\\nSearch query example:\")\n",
        "result = search_query(\"mahasiswa\")\n",
        "print(result[['TIDAK BAKU', 'BAKU']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhFMiJQ7nYQ-",
        "outputId": "aadabbce-d249-4f98-9975-fdcb4726be9f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized words:\n",
            "0         [mangtab]\n",
            "1     [evolusionis]\n",
            "2        [abstarck]\n",
            "3    [sebenenarnya]\n",
            "4     [kataakanlah]\n",
            "Name: tokens, dtype: object\n",
            "\n",
            "Filtered words:\n",
            "0         [mangtab]\n",
            "1     [evolusionis]\n",
            "2        [abstarck]\n",
            "3    [sebenenarnya]\n",
            "4     [kataakanlah]\n",
            "Name: filtered_tokens, dtype: object\n",
            "\n",
            "Stemmed words:\n",
            "0         [mangtab]\n",
            "1     [evolusionis]\n",
            "2        [abstarck]\n",
            "3    [sebenenarnya]\n",
            "4     [kataakanlah]\n",
            "Name: stemmed_tokens, dtype: object\n",
            "\n",
            "Index (first 10 terms):\n",
            "['sebanrnya', 'omdo', 'hubnya', 'sebenenarnya', 'semster', 'gua', 'berkwalitas', 'ntn', 'taeeekkk', 'plis']\n",
            "\n",
            "TF-IDF Matrix shape:\n",
            "(42, 38)\n",
            "\n",
            "Search query example:\n",
            "   TIDAK BAKU       BAKU\n",
            "36   mahasiwa  mahasiswa\n",
            "41     gercep    tangkas\n",
            "9       kanca     kancah\n",
            "17      Alloh      Allah\n",
            "16   intgrasi  integrasi\n"
          ]
        }
      ]
    }
  ]
}